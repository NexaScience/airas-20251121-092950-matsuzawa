Downloading cpython-3.11.14-linux-x86_64-gnu (download) (29.1MiB)
 Downloaded cpython-3.11.14-linux-x86_64-gnu (download)
Using CPython 3.11.14
Creating virtual environment at: .venv
Resolved 128 packages in 332ms
Downloading nvidia-cusparselt-cu12 (273.9MiB)
Downloading networkx (1.9MiB)
Downloading nvidia-cufft-cu12 (184.2MiB)
Downloading nvidia-nvshmem-cu12 (118.9MiB)
Downloading pyarrow (45.5MiB)
Downloading nvidia-nvjitlink-cu12 (37.4MiB)
Downloading numpy (16.1MiB)
Downloading pandas (12.2MiB)
Downloading transformers (11.4MiB)
Downloading pillow (6.7MiB)
Downloading sympy (6.0MiB)
Downloading fonttools (4.7MiB)
Downloading hf-xet (3.2MiB)
Downloading sqlalchemy (3.2MiB)
Downloading tokenizers (3.1MiB)
Downloading pydantic-core (2.0MiB)
Downloading aiohttp (1.7MiB)
Downloading kiwisolver (1.4MiB)
Downloading nvidia-cufile-cu12 (1.1MiB)
Downloading bitsandbytes (56.6MiB)
Downloading torch (858.1MiB)
Downloading triton (162.5MiB)
Downloading nvidia-cuda-cupti-cu12 (9.8MiB)
Downloading scipy (34.2MiB)
Downloading nvidia-cublas-cu12 (566.8MiB)
Downloading nvidia-cusparse-cu12 (274.9MiB)
Downloading nvidia-curand-cu12 (60.7MiB)
Downloading matplotlib (8.3MiB)
Downloading nvidia-cuda-nvrtc-cu12 (84.0MiB)
Downloading nvidia-cusolver-cu12 (255.1MiB)
Downloading nvidia-nccl-cu12 (307.4MiB)
Downloading nvidia-cudnn-cu12 (674.0MiB)
Downloading wandb (19.3MiB)
 Downloaded nvidia-cufile-cu12
 Downloaded kiwisolver
 Downloaded aiohttp
 Downloaded pydantic-core
 Downloaded tokenizers
 Downloaded hf-xet
 Downloaded sqlalchemy
 Downloaded networkx
 Downloaded fonttools
 Downloaded pillow
 Downloaded matplotlib
 Downloaded sympy
 Downloaded nvidia-cuda-cupti-cu12
 Downloaded numpy
 Downloaded transformers
 Downloaded wandb
 Downloaded pandas
 Downloaded nvidia-nvjitlink-cu12
 Downloaded scipy
 Downloaded bitsandbytes
 Downloaded nvidia-curand-cu12
 Downloaded pyarrow
 Downloaded nvidia-cuda-nvrtc-cu12
 Downloaded nvidia-nvshmem-cu12
 Downloaded nvidia-cufft-cu12
 Downloaded triton
 Downloaded nvidia-cusolver-cu12
 Downloaded nvidia-cusparse-cu12
 Downloaded nvidia-cusparselt-cu12
 Downloaded nvidia-nccl-cu12
 Downloaded nvidia-cublas-cu12
 Downloaded nvidia-cudnn-cu12
 Downloaded torch
Prepared 97 packages in 32.19s
Installed 97 packages in 337ms
 + accelerate==1.12.0
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.2
 + aiosignal==1.4.0
 + alembic==1.17.2
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + bitsandbytes==0.48.2
 + certifi==2025.11.12
 + charset-normalizer==3.4.4
 + click==8.3.1
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.4.1
 + dill==0.4.0
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.10.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.18
 + networkx==3.5
 + numpy==2.3.5
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.6.0
 + packaging==25.0
 + pandas==2.3.3
 + peft==0.18.0
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.1
 + psutil==7.1.3
 + pyarrow==22.0.0
 + pydantic==2.12.4
 + pydantic-core==2.41.5
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.11.3
 + requests==2.32.5
 + safetensors==0.7.0
 + scipy==1.16.3
 + seaborn==0.13.2
 + sentry-sdk==2.45.0
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + tokenizers==0.22.1
 + torch==2.9.1
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.1
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + urllib3==2.5.0
 + wandb==0.23.0
 + xxhash==3.6.0
 + yarl==1.22.0
/home/runner/work/airas-20251121-092950-matsuzawa/airas-20251121-092950-matsuzawa/.venv/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:916: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['lm_head'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.
  warnings.warn(
Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]Generating train split: 100%|██████████| 7473/7473 [00:00<00:00, 577387.06 examples/s]
Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 1319/1319 [00:00<00:00, 482494.94 examples/s]
Map:   0%|          | 0/7473 [00:00<?, ? examples/s]Map:   5%|▍         | 361/7473 [00:00<00:01, 3581.42 examples/s]Map:  10%|▉         | 739/7473 [00:00<00:01, 3689.10 examples/s]Map:  17%|█▋        | 1238/7473 [00:00<00:01, 3481.55 examples/s]Map:  22%|██▏       | 1619/7473 [00:00<00:01, 3593.44 examples/s]Map:  27%|██▋       | 2000/7473 [00:00<00:01, 3461.85 examples/s]Map:  32%|███▏      | 2390/7473 [00:00<00:01, 3594.57 examples/s]Map:  37%|███▋      | 2785/7473 [00:00<00:01, 3701.14 examples/s]Map:  44%|████▍     | 3307/7473 [00:00<00:01, 3609.28 examples/s]Map:  49%|████▉     | 3693/7473 [00:01<00:01, 3674.82 examples/s]Map:  56%|█████▌    | 4199/7473 [00:01<00:00, 3560.57 examples/s]Map:  61%|██████▏   | 4587/7473 [00:01<00:00, 3641.06 examples/s]Map:  67%|██████▋   | 4981/7473 [00:01<00:00, 3720.12 examples/s]Map:  74%|███████▎  | 5493/7473 [00:01<00:00, 3603.80 examples/s]Map:  79%|███████▊  | 5875/7473 [00:01<00:00, 3656.89 examples/s]Map:  85%|████████▌ | 6386/7473 [00:01<00:00, 3548.12 examples/s]Map:  91%|█████████ | 6775/7473 [00:01<00:00, 3632.16 examples/s]Map:  98%|█████████▊| 7291/7473 [00:02<00:00, 3560.08 examples/s]Map: 100%|██████████| 7473/7473 [00:02<00:00, 3585.76 examples/s]
Map:   0%|          | 0/1319 [00:00<?, ? examples/s]Map:  29%|██▉       | 387/1319 [00:00<00:00, 3839.85 examples/s]Map:  59%|█████▉    | 777/1319 [00:00<00:00, 3874.20 examples/s]Map:  97%|█████████▋| 1274/1319 [00:00<00:00, 3554.74 examples/s]Map: 100%|██████████| 1319/1319 [00:00<00:00, 3551.42 examples/s]
Epoch 1/1:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 1/1:  12%|█▎        | 2/16 [00:00<00:00, 18.25it/s]Epoch 1/1:  25%|██▌       | 4/16 [00:00<00:00, 17.23it/s]Epoch 1/1:  44%|████▍     | 7/16 [00:00<00:00, 19.74it/s]Epoch 1/1:  62%|██████▎   | 10/16 [00:00<00:00, 19.91it/s]Epoch 1/1:  81%|████████▏ | 13/16 [00:00<00:00, 19.64it/s]Epoch 1/1:  94%|█████████▍| 15/16 [00:00<00:00, 18.58it/s]Epoch 1/1: 100%|██████████| 16/16 [00:00<00:00, 18.65it/s]
/home/runner/work/airas-20251121-092950-matsuzawa/airas-20251121-092950-matsuzawa/.venv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Downloading cpython-3.11.14-linux-x86_64-gnu (download) (29.1MiB)
 Downloaded cpython-3.11.14-linux-x86_64-gnu (download)
Using CPython 3.11.14
Creating virtual environment at: .venv
Resolved 128 packages in 584ms
Downloading torch (858.1MiB)
Downloading nvidia-cudnn-cu12 (674.0MiB)
Downloading nvidia-cublas-cu12 (566.8MiB)
Downloading nvidia-nccl-cu12 (307.4MiB)
Downloading nvidia-cusparse-cu12 (274.9MiB)
Downloading nvidia-cusparselt-cu12 (273.9MiB)
Downloading nvidia-cusolver-cu12 (255.1MiB)
Downloading triton (162.5MiB)
Downloading nvidia-nvshmem-cu12 (118.9MiB)
Downloading nvidia-cuda-nvrtc-cu12 (84.0MiB)
Downloading nvidia-curand-cu12 (60.7MiB)
Downloading bitsandbytes (56.6MiB)
Downloading pyarrow (45.5MiB)
Downloading nvidia-nvjitlink-cu12 (37.4MiB)
Downloading scipy (34.2MiB)
Downloading wandb (19.3MiB)
Downloading numpy (16.1MiB)
Downloading pandas (12.2MiB)
Downloading transformers (11.4MiB)
Downloading nvidia-cuda-cupti-cu12 (9.8MiB)
Downloading matplotlib (8.3MiB)
Downloading pillow (6.7MiB)
Downloading sympy (6.0MiB)
Downloading fonttools (4.7MiB)
Downloading hf-xet (3.2MiB)
Downloading sqlalchemy (3.2MiB)
Downloading tokenizers (3.1MiB)
Downloading pydantic-core (2.0MiB)
Downloading networkx (1.9MiB)
Downloading aiohttp (1.7MiB)
Downloading kiwisolver (1.4MiB)
Downloading nvidia-cufile-cu12 (1.1MiB)
Downloading nvidia-cufft-cu12 (184.2MiB)
 Downloaded nvidia-cufile-cu12
 Downloaded kiwisolver
 Downloaded aiohttp
 Downloaded pydantic-core
 Downloaded tokenizers
 Downloaded hf-xet
 Downloaded networkx
 Downloaded sqlalchemy
 Downloaded fonttools
 Downloaded sympy
 Downloaded pillow
 Downloaded matplotlib
 Downloaded nvidia-cuda-cupti-cu12
 Downloaded transformers
 Downloaded pandas
 Downloaded numpy
 Downloaded wandb
 Downloaded scipy
 Downloaded nvidia-nvjitlink-cu12
 Downloaded pyarrow
 Downloaded bitsandbytes
 Downloaded nvidia-curand-cu12
 Downloaded nvidia-cuda-nvrtc-cu12
 Downloaded nvidia-nvshmem-cu12
 Downloaded triton
 Downloaded nvidia-cufft-cu12
 Downloaded nvidia-cusolver-cu12
 Downloaded nvidia-cusparse-cu12
 Downloaded nvidia-cusparselt-cu12
 Downloaded nvidia-nccl-cu12
 Downloaded nvidia-cublas-cu12
 Downloaded nvidia-cudnn-cu12
 Downloaded torch
Prepared 97 packages in 40.04s
Installed 97 packages in 293ms
 + accelerate==1.12.0
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.2
 + aiosignal==1.4.0
 + alembic==1.17.2
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + bitsandbytes==0.48.2
 + certifi==2025.11.12
 + charset-normalizer==3.4.4
 + click==8.3.1
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.4.1
 + dill==0.4.0
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.10.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.18
 + networkx==3.5
 + numpy==2.3.5
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.6.0
 + packaging==25.0
 + pandas==2.3.3
 + peft==0.18.0
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.1
 + psutil==7.1.3
 + pyarrow==22.0.0
 + pydantic==2.12.4
 + pydantic-core==2.41.5
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.11.3
 + requests==2.32.5
 + safetensors==0.7.0
 + scipy==1.16.3
 + seaborn==0.13.2
 + sentry-sdk==2.45.0
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + tokenizers==0.22.1
 + torch==2.9.1
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.1
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + urllib3==2.5.0
 + wandb==0.23.0
 + xxhash==3.6.0
 + yarl==1.22.0
/home/runner/work/airas-20251121-092950-matsuzawa/airas-20251121-092950-matsuzawa/.venv/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:916: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['lm_head'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.
  warnings.warn(
Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]Generating train split: 100%|██████████| 7473/7473 [00:00<00:00, 632267.60 examples/s]
Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 1319/1319 [00:00<00:00, 411781.69 examples/s]
Map:   0%|          | 0/7473 [00:00<?, ? examples/s]Map:   5%|▍         | 351/7473 [00:00<00:02, 3481.68 examples/s]Map:  10%|▉         | 723/7473 [00:00<00:01, 3616.89 examples/s]Map:  16%|█▌        | 1209/7473 [00:00<00:01, 3403.16 examples/s]Map:  21%|██        | 1585/7473 [00:00<00:01, 3525.46 examples/s]Map:  27%|██▋       | 2050/7473 [00:00<00:01, 3340.66 examples/s]Map:  33%|███▎      | 2438/7473 [00:00<00:01, 3490.87 examples/s]Map:  38%|███▊      | 2830/7473 [00:00<00:01, 3613.83 examples/s]Map:  45%|████▌     | 3380/7473 [00:00<00:01, 3523.25 examples/s]Map:  50%|█████     | 3759/7473 [00:01<00:01, 3589.27 examples/s]Map:  57%|█████▋    | 4248/7473 [00:01<00:00, 3468.08 examples/s]Map:  62%|██████▏   | 4632/7473 [00:01<00:00, 3560.14 examples/s]Map:  67%|██████▋   | 5000/7473 [00:01<00:00, 3449.28 examples/s]Map:  72%|███████▏  | 5381/7473 [00:01<00:00, 3544.72 examples/s]Map:  77%|███████▋  | 5763/7473 [00:01<00:00, 3619.52 examples/s]Map:  84%|████████▎ | 6251/7473 [00:01<00:00, 3481.40 examples/s]Map:  89%|████████▊ | 6632/7473 [00:01<00:00, 3566.34 examples/s]Map:  94%|█████████▎| 7000/7473 [00:02<00:00, 3426.51 examples/s]Map:  99%|█████████▊| 7372/7473 [00:02<00:00, 3502.99 examples/s]Map: 100%|██████████| 7473/7473 [00:02<00:00, 3492.68 examples/s]
Map:   0%|          | 0/1319 [00:00<?, ? examples/s]Map:  28%|██▊       | 374/1319 [00:00<00:00, 3711.55 examples/s]Map:  57%|█████▋    | 757/1319 [00:00<00:00, 3779.41 examples/s]Map:  95%|█████████▍| 1250/1319 [00:00<00:00, 3495.90 examples/s]Map: 100%|██████████| 1319/1319 [00:00<00:00, 3484.04 examples/s]
Epoch 1/1:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 1/1:  12%|█▎        | 2/16 [00:00<00:00, 16.97it/s]Epoch 1/1:  25%|██▌       | 4/16 [00:00<00:00, 16.06it/s]Epoch 1/1:  44%|████▍     | 7/16 [00:00<00:00, 18.60it/s]Epoch 1/1:  62%|██████▎   | 10/16 [00:00<00:00, 19.06it/s]Epoch 1/1:  75%|███████▌  | 12/16 [00:00<00:00, 18.72it/s]Epoch 1/1:  88%|████████▊ | 14/16 [00:00<00:00, 17.06it/s]Epoch 1/1: 100%|██████████| 16/16 [00:00<00:00, 16.89it/s]Epoch 1/1: 100%|██████████| 16/16 [00:00<00:00, 17.48it/s]
/home/runner/work/airas-20251121-092950-matsuzawa/airas-20251121-092950-matsuzawa/.venv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
